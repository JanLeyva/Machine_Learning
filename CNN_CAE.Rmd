---
title: "**CNNs - Malaria image recogenization problem** \n \n Universitat Polit√®cnica de Catalunya"
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
author: '`r params$author`'
output:
  html_document:
    theme: united
    df_print: paged
    toc: yes
    toc_float: yes
  pdf_document:
    number_sections: yes
    toc: yes
params:
  show_code: TRUE
  seed: 1234
  author: 'Geraldo Gariza, Jan Leyva and Andreu Meca'
  #partition: 0.6666666666666666666667
  myDescription: 'Malaria is a deadly, infectious mosquito-borne disease caused by Plasmodium parasites. These parasites are transmitted by the bites of infected female Anopheles mosquitoes. With regular manual diagnosis of blood smears, it is an intensive manual process requiring proper expertise in classifying and counting the parasitized and uninfected cells. Typically this may not scale well and might cause problems if we do not have the right expertise in specific regions around the world. We are lucky to have researchers at the Lister Hill National Center for Biomedical Communications (LHNCBC), part of National Library of Medicine (NLM) who have carefully collected and annotated this dataset of healthy and infected blood smear images. There is a balanced dataset of 13779 malaria and non-malaria (uninfected) cell images. The dataset consist of near thousand images downloaded from the official website that have been resized (64x64x3). The images are organized in three folders: train, validation and test. Deep Learning models, or to be more specific, Convolutional Neural Networks (CNNs) have proven to be really effective in a wide variety of computer vision tasks.'
  #dataset: 
#bibliography: scholar.bib  
---


\newpage

```{r setup_rmd, include=FALSE,echo=FALSE}
knitr::opts_chunk$set(echo = params$show_code, error = TRUE, warning = FALSE)
```


```{r packages, include=FALSE}
# If the package is not installed then it will be installed
if(!require("knitr")) install.packages("knitr")
if(!require("keras")) install.packages("keras")
if(!require("kerasR")) install.packages("kerasR")
if(!require("tfruns")) install.packages("tfruns")
if(!require("caret")) install.packages("caret")
if(!require("e1071")) install.packages("e1071")
library("knitr")
library("keras")
library("kerasR")
library("tfruns")
library("caret")
library("e1071")
library("grDevices")
library("raster")
```

```{r}
# reticulate::py_install("pillow")
# use_python("/Library/Frameworks/Python.framework/Versions/3.9/bin/python3")
# use_python("~/local/bin/python3")
```


# Introduction


`r params$myDescription`. 


The first step is to charge the directories where the data is stored, which is inside three folders called `train`, `test` and `validation`

```{r import data, include=FALSE}
base_dir<-"~/KOLMOGOROV/Assignment_CNN/malaria/malaria"

# train directories
train_dir<-file.path(base_dir,"train",fsep ="/")
train_dir_infected<-file.path(train_dir,"infected",fsep = "/")
train_dir_uninfected<-file.path(train_dir,"uninfected",fsep = "/")

# vaidation directories
validation_dir<-file.path(base_dir,"validation",fsep ="/")
validation_dir_infected<-file.path(validation_dir,"infected",fsep = "/")
validation_dir_uninfected<-file.path(validation_dir,"uninfected",fsep = "/")

# test directories
test_dir<-file.path(base_dir,"test",fsep ="/")
test_dir_infected<-file.path(test_dir,"infected",fsep = "/")
test_dir_uninfected<-file.path(test_dir,"uninfected",fsep = "/")
```

Once the data is loaded, it is resized into a generator object.

```{r data importation 0}
# image_data_generator Generate batches of image data with real-time data augmentation. The data will be looped over (in batches).
train_datagen <- image_data_generator(rescale = 1/255) #
validation_datagen <- image_data_generator(rescale = 1/255) #
test_datagen <- image_data_generator(rescale = 1/255) #

#flow_images_from_directory Generates batches of data from images in a directory (with optional augmented/normalized data)
train_generator <- flow_images_from_directory(
train_dir,
train_datagen,
target_size = c(64, 64),
batch_size = 1,
class_mode = "binary"
)
validation_generator <- flow_images_from_directory(
validation_dir,
validation_datagen,
target_size = c(64, 64),
batch_size = 1,
class_mode = "binary"
)


test_datagen <- image_data_generator(rescale = 1/255) #

test_generator <- flow_images_from_directory(
test_dir,
test_datagen,
target_size = c(64, 64),
batch_size = 1,
class_mode = "binary"
)
batch <- generator_next(train_generator)
str(batch) 
```

Now, we have loaded 1001 images for train, 420 for validation and 514 to test all with two classes. Also all are of dimension 64x64x3 and, in case that one of the images is not of this exact size, the generator will reshape it to match the correct dimension.

# Section 1 - CNN

## Implement a CNN according with the summary in Figure 1

To implement the Convolutional Neural Network we'll use the function keras_model_sequential() as well as the layer_conv_2d(), layer_max_pooling_2d(), layer_flatten() and layer_dense() to replicate the CNN in Figure 1

```{r cnn def}
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
                input_shape = c(64, 64, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

To see if it is the same as in Figure 1 we call the summary() function and check that it indeed has the same structure
 
```{r cnn summary}
summary(model)
```

# Section 2 - CNN definition

## Define conveniently the model (optimization, loss, metric, ...)

In this step the function used is the compile() function, in which three parameters are passed to configure the loss function, optimizer and the metric to compute, which are, respectively, the binary crossentropy function, the RMSprop optimizer with a learning rate of 0.0004 and accuracy. The binary crossentropy function is defined as follows:

Binary Cross-Entropy $= -\frac{1}{N}\sum_{i = 1}^{N}{(y_ilog(p(y_i))+(1-y_i)log(1-p(y_i))}$

This is the chosen loss function as the aim of this CNN is predict two categories (malaria or not malaria)

```{r}
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("accuracy")
)
```


```{r}
history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 1000,
  epochs = 30, 
  validation_data = validation_generator,
  validation_steps = 50
)
```

In order to visualize the model performance a plot of the performance for each epoch of the CNN is generated

```{r history performance}
plot(history)
```




To save the model we use the save_model_hdf5 and to load a model the load_model_hdf5

```{r save_model_hdf5}
model %>% save_model_hdf5("cnn.h5")
```


```{r load_model_hdf5}
# model_load <- load_model_hdf5("cnn.h5")
```


# Section 3 - Tunning the hyperparameter

## Using tfruns package tune the hyperparameter batch_size exploring the grid c(16, 32, 64)

In this section the hyperparameter `batch_size` is tuned to explore the c(16, 32, 64) grid using the function flags() to set the flag hyperparameter

```{r flags}
FLAGS <- flags(
   flag_string("hl1", "batch size"))
```

In the next chunk the model is re-defined to introduce the flags in order to tune the hyperparameter

```{r Tunning hyperparameter}
model %>% fit(train_generator, train_generator$labels, epochs = 10, 
               batch_size = FLAGS$hl1)
```

Training the model that is stored in `cnn_assignment.R` with the different `batch size` in a loop.

```{r}
for (hl1 in c(16, 32, 64)){
training_run("cnn_assignment.R", flags = c(hl1 = hl1))
}
```

If we want see the performance of the runs that we did before:

```{r view runs, eval=FALSE}
View(ls_runs())
```

In order to compare the last two models:

```{r compare, eval=FALSE}
compare_runs()
```

# Section 4 - Early-stopping callbacks()

## Implement an early-stopping callbacks() to interrupt training when validation accuracy stops improving for more than two epochs

In this section an early-stopping is implemented to stop the training and validation accuracy when the model stops improving for more than two epochs. In this step we used `EarlyStopping` as a function inside `callbacks`. We specify `patience` = 3 to stop the training when the `accuracy` stops improving for *more than two epochs*.

```{r callback, include=FALSE}
model %>% fit(train_generator,
              epochs = 30,
              callbacks = list(callback_model_checkpoint("cnn.h5"),
                               EarlyStopping(monitor = "accuracy",
                                             patience = 3)))
```


# Section 5 - Predict

## Assess the performance of the CNN predicting the categories of test images and obtain the confusion matrix

```{r evaluate}
# evaluate
scores <- model %>% evaluate(test_generator, 
                             test_generator$labels, 
                             verbose = 0)

# Output metrics
cat('Test loss:', scores[[1]], '\n')
cat('Test accuracy:', scores[[2]], '\n')
```


As we did before with the training and validation sets, the vector containing the test images is loaded into a new object where the test data is reshaped to have a 64x64x3 dimension

```{r prediction}
y_pred <-model %>%
            predict(test_generator)

y_pred_ <- c()
for(i in 1:length(y_pred)){
  ifelse(y_pred[i] > 0.50001, y_pred_[i] <- 1, y_pred_[i] <- 0)
}

confusionMatrix(as.factor(y_pred_), as.factor(test_generator$labels))
```

This confusion is not correct, we detected diferences using predict with the generator and the `evaluate` function used before. For this reason we try to import the images and store it in array of dimension (514, 64, 64, 3). As we have now the images in array instead of a generator we can use the function `predict_classes` that return the class predicted by CNN. This time the confusion matrix return the same result that we obtaint before with `evaluate`.

```{r array of images test}
fnames_inf   <- list.files(test_dir_infected, full.names = TRUE)
fnames_uninf <- list.files(test_dir_uninfected, full.names = TRUE)
files_names  <-c(fnames_inf, fnames_uninf)

img_array <- array(dim = c(length(files_names),64,64,3)) # dimension of the array including images
for(i in 1:length(files_names)){
  img<-image_load(files_names[i])
  img_tensor<-image_to_array(img)
  img_tensor<-array_reshape(img_tensor,c(1,64,64,3))
  img_array[i,,,]<-img_tensor/255
}
dim(img_array)
```


```{r vector with label}
test_labels<-as.factor(c( rep(0, length(fnames_inf)), rep(1, length(fnames_uninf)) ))
head(test_labels)
```

```{r images prediction}
predict_test <- model %>% 
                      predict_classes(img_array)
```

Keras doesn't include a built-in Confusion Matrix, therefore the `caret` package is loaded to make use of the `confusionMatrix` function. First of all, we transform the probabilities greater than 0.5 to class 1, else to 0. We did this because the image object is a generator and the function `predict_classes` does not accept generators, only vectors list or atomics. 

```{r prediction with array}
confusionMatrix(as.factor(predict_test), test_labels)
```

The result shows us the what we expected, the model overfits the training data set, as we could see on the first *history* plot, while the train set accuracy improves its performance linearly the validation set is constant and it even decreases. This is a clear sign of overfitting. 

For futures works, it would be interesting to apply different techniques to mitigate the overfitting such as dropout, regularization technique used to change the node architecture in every iteration, weight decay which is also called L2 regularization, technique that introduces a penalization parameter, or use data augmentation to increase the variety of data used in the model training step.

# Section 6 - Convolutional AutoEncoder (CAE)

## Implement two Convolutional AutoEncoder (CAE). The first with 3 nodes in z layer (or bottleneck), the second with 10 nodes in z layer. Feel free to choose the number of convolutional layers, filter sizes, number of filters:

An autoencoder is a neural network designed to learn a representation of a set of data and ignore the signal noise. It has an encoder and a decoder part, the first one to reduce the dimension of the data and the second one to reconstruct the input as well as possible.

First of all, we import the images again but this time with the `class_mode = input`. This is really important in order to get a results with sens. This solutions is found in the following discussion [link](https://stackoverflow.com/questions/51669382/keras-dimension-mismatch-in-last-layer-of-autoencoder/51673998#51673998).

```{r data importation, include=FALSE}
# image_data_generator Generate batches of image data with real-time data augmentation. The data will be looped over (in batches).
train_datagen <- image_data_generator(rescale = 1/255) #
validation_datagen <- image_data_generator(rescale = 1/255) #
test_datagen <- image_data_generator(rescale = 1/255) #

#flow_images_from_directory Generates batches of data from images in a directory (with optional augmented/normalized data)
train_generator <- flow_images_from_directory(
train_dir,
train_datagen,
target_size = c(64, 64),
batch_size = 1,
class_mode = "input"
)
validation_generator <- flow_images_from_directory(
validation_dir,
validation_datagen,
target_size = c(64, 64),
batch_size = 1,
class_mode = "input"
)


test_datagen <- image_data_generator(rescale = 1/255) #

test_generator <- flow_images_from_directory(
test_dir,
test_datagen,
target_size = c(64, 64),
batch_size = 1,
class_mode = "input"
)
batch <- generator_next(train_generator)
str(batch) 
```

Let's build the first Convolutional Autoencoder, which it only has the requirement of having 3 nodes in layer z.

The encoder is built with 3 convolutional layers, where the input dimension, as said, is 64x64x3 and the output is 3x3x3. The `stride` parameter in all filters is set to (1, 1) and the `padding` to same, so that the output of the filter has the same dimension as the input.

### Convolutional Encoder
```{r Convolutional Encoder}
input_dim <- c(64, 64, 3)
model_enc <- keras_model_sequential() 
model_enc %>%
  layer_conv_2d(filters = 3, kernel_size = c(3,3), 
                activation = "relu", padding = "same",
                input_shape = input_dim)  %>%
  layer_max_pooling_2d(pool_size = c(4,4), padding = "same")  %>%
  layer_conv_2d(filters = 12, kernel_size = c(3,3), 
                activation = "relu", padding = "same") %>%
  layer_max_pooling_2d(pool_size = c(3,3), padding = "same")  %>%
  layer_conv_2d(filters = 24, kernel_size = c(3,3), 
                activation = "relu", padding = "same")%>%
  layer_max_pooling_2d(pool_size = c(2,2), padding = "same")  %>%
  layer_conv_2d(filters = 3, kernel_size = c(3,3), 
                activation = "relu", padding = "same")

summary(model_enc)
```

The Convolutional Decoder is built with a structure of 5 convolutional layers, the first four with the ReLU function and the last one with a sigmoid activation function. The inputs of the decoder have a dimension of 3x3x3 and the output, as in the input of the decoder, a dimension of 64x64x3. 

### Convolutional Decoder
```{r Convolutional Decoder}
model_dec <- keras_model_sequential() 
model_dec %>%
  layer_conv_2d(filters = 3, kernel_size = c(3,3), 
                activation = "relu", padding = "same",
                input_shape =c(3,3, 3) )  %>%
  layer_upsampling_2d(size = c(2,2))%>%
  layer_conv_2d(24,kernel_size = c(3,3), activation = "relu",padding = "same")%>%
  layer_upsampling_2d(size =  c(2,2))%>%
  layer_conv_2d(48, kernel_size =c(3,3), activation = "relu")%>%
  layer_upsampling_2d(size =  c(2,2))%>%
  layer_conv_2d(24, kernel_size =c(3,3), activation = "relu" )%>%
  layer_conv_2d(3, kernel_size =c(3,3), activation = "sigmoid" )%>%
  layer_upsampling_2d(size =  c(4,4))

summary(model_dec)
```

The loss function chosen is the binary cross-entropy, the same that was used for the Convolutional Neural Network in Section 2, but now the optimizer is adam and the metric is the mean squared error.

```{r compile model}
CAE<-keras_model_sequential()
CAE %>%model_enc%>%model_dec

summary(CAE)

CAE %>% compile(
  loss = "binary_crossentropy",
  #optimizer = optimizer_rmsprop(),
  optimizer = "adam",
  metrics = c("mean_squared_error")
)
```

```{r train the CAE}
history <- CAE %>% fit(
  x= train_generator , y = train_generator ,   # Autoencoder
  epochs = 10, 
  suffle = TRUE
  #validation_data = validation_generator
)

plot(history)
```


* Plot original images
Let's plot the original image and compare the input and the output of the Autoencoder so that we can see the differences

```{r riginal image}
op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))
for (i in 1:4){
plot(as.raster(img_array[i,,,]))
}  
```

* Predict and plot image from CAE output

Now, the prediction of the CAE is computed with the function `predict()`

```{r predict CAE}
output_cifra<-predict(CAE, test_generator)
dim(output_cifra)
```

```{r plot image predicted}
op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))
for (i in 1:4){
plot(as.raster((output_cifra[i,,,])))
}
```

* In gray scale

```{r}
op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))
for (i in 1:4){
im<-matrix(output_cifra[i,,,1], nrow=64, ncol=64) 
image(1:64, 1:64, im, col=gray((0:255)/255))
}
```

* Plot images from encoder output

```{r From encoder to decoder1}
enc_output_cifra<-predict(model_enc, train_generator)
dim(enc_output_cifra)
```


```{r plot From encoder to decoder2}
op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))
for (i in 1:4){
plot(as.raster(enc_output_cifra[1,,,]/10))
}
```

* From encoder to decoder

```{r From encoder to decoder}
dec_output_cifra<-predict(model_dec, enc_output_cifra)
dim(dec_output_cifra)
```


```{r plot From encoder to decoder}
op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))
for (i in 1:4){
plot(as.raster(dec_output_cifra[i,,,]))
}
```



### Convolutional AutoEncoder z = 10

Now, it's time for the second Convolutional AutoEncoder, with the requirement to have 10 nodes in the z layer.

The encoder is built with 3 convolutional layers, where the input dimension, as said, is 64x64x12 and the output is 10x10x3. The `stride` parameter in all filters is set to (1, 1) and the `padding` to same.

* CAE encoder size 10
```{r CAE encoder size 10}
#### Convolutional Encoder 
model_enc10 <- keras_model_sequential() 
model_enc10 %>%
  layer_conv_2d(filters = 12, kernel_size = c(3,3), 
                activation = "relu", padding = "same",
                input_shape = input_dim)  %>%
  layer_max_pooling_2d(pool_size = c(2,2), padding = "same")  %>%
  layer_conv_2d(filters = 8, kernel_size = c(3,3), 
                activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(3,3), padding = "same")  %>%
  layer_conv_2d(filters = 3, kernel_size = c(3,3), 
                activation = "relu", padding = "same")

summary(model_enc10)
```

The Convolutional Decoder is built with a structure of 3 convolutional layers, the first two with the ReLU function and the last one, as we did before, with a sigmoid activation function. The inputs of the decoder have a dimension of 10x10x3 and the output, as in the input of the decoder, a dimension of 64x64x3


* CAE decoder size 10

```{r CAE decoder size 10}

model_dec10 <- keras_model_sequential() 
model_dec10 %>%
  layer_conv_2d(filters = 3, kernel_size = c(3,3), 
                activation = "relu", padding = "same",
                input_shape =c(10,10,3) )  %>%
  layer_upsampling_2d(size = c(2,2)) %>%
  layer_conv_2d(filters = 4, kernel_size = c(3,3), 
                activation = "relu")%>%
  layer_conv_2d(filters = 3, kernel_size = c(3,3), 
                activation = "sigmoid")%>%
  layer_upsampling_2d(size = c(4,4)) %>%
  

summary(model_dec10)
```

* Join the encoder and decoder models
```{r}
model10<-keras_model_sequential()
model10 %>% model_enc10 %>%model_dec10
```

The parameters for the `compile()` function are the same as the first CAE, as the goal of it is the also the same

```{r}
model10 %>% compile(
  loss = "mean_squared_error",
  #optimizer = optimizer_rmsprop(),
  optimizer = "adam",
  metrics = c("mean_squared_error")
)

history <- model10 %>% fit(
  x=train_generator , y =train_generator,   # Autoencoder
  epochs = 5, 
  suffle = TRUE
#  validation_data = list(x_test_cifra,x_test_cifra)
)
```

### Prediction of CAE (size 10)

* Plot output CAE 10
```{r predictions CAE 10}
output_cifra_10 <-predict(model10, test_generator)
dim(output_cifra_10)
```

```{r plot results CAE 10}
op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))
for (i in 1:4){
plot(as.raster(output_cifra_10[i,,,]))
}
```


* Plot results CAE 10 encoder
```{r predict results CAE 10 encoder}
enc_output_cifra_10 <-predict(model_enc, test_generator)
dim(enc_output_cifra_10)
```

```{r plot results CAE 10 encoder}
op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))
for (i in 1:4){
plot(as.raster(enc_output_cifra_10[i,,,]/10))
}
```


* Plot output CAE 10 decoder
```{r predict results CAE 10 decoder}
dec_output_cifra_10 <-predict(model_dec, enc_output_cifra_10)
dim(dec_output_cifra_10)
```

```{r plot results CAE 10 encoder 2}
op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))
for (i in 1:4){
plot(as.raster(dec_output_cifra_10[i,,,]))
}
```

# Section 7

## Represent graphically the results from images test to show the association between z layer activations and the class images. Compare the representations displayed by both CAE architectures

Let's plot the output of both AutoEncoders to check if there are any differences the naked eye is able to see

```{r comparison between 10 vs 3}
op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))
for (i in 1:2){
plot(as.raster(dec_output_cifra[i,,,]), main = "CAE z = 3")
}
for (i in 1:2){
plot(as.raster(dec_output_cifra_10[i,,,]), main = "CAE z = 10")
}
```

Looking at both representations there is not much of a difference at first sight, but if we look closely, it is possible to see that some pixels differ from one another. The majority of the pixels that are different can be found at the edges of the main pinkish figure, that in contrast with the black background around it, the pixels in the border change the tone of the pink palette comparing both visualizations.
